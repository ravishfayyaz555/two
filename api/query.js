// Advanced AI-Powered Chatbot API for Physical AI Textbook
// Intelligent, context-aware responses with comprehensive topic coverage
<<<<<<< HEAD
// Supports general questions, context-specific queries, chapter-aware prioritization, and educational explanations
//
// Security: Input sanitization, output encoding, rate limiting ready
// Educational: Tone enforcement, structure validation, word limits

/**
 * Sanitize user input to prevent XSS and injection attacks
 * @param {string} input - Raw user input
 * @param {number} maxLength - Maximum allowed length (default: 1000)
 * @returns {string} - Sanitized input
 */
function sanitizeInput(input, maxLength = 1000) {
  if (typeof input !== 'string') {
    return '';
  }
  // Trim and limit length
  let sanitized = input.trim().substring(0, maxLength);
  // Remove potential XSS patterns
  sanitized = sanitized
    .replace(/<script\b[^<]*(?:(?!<\/script>)<[^<]*)*<\/script>/gi, '')
    .replace(/javascript:/gi, '')
    .replace(/on\w+=/gi, '')
    .replace(/<iframe/gi, '[iframe blocked]')
    .replace(/<object/gi, '[object blocked]')
    .replace(/<embed/gi, '[embed blocked]');
  return sanitized;
}

/**
 * Escape markdown special characters for safe display
 * @param {string} text - Text to escape
 * @returns {string} - Escaped text
 */
function escapeForMarkdown(text) {
  if (typeof text !== 'string') {
    return '';
  }
  return text
    .replace(/</g, '&lt;')
    .replace(/>/g, '&gt;')
    .replace(/`/g, '\\`')
    .replace(/\*/g, '\\*')
    .replace(/_/g, '\\_')
    .replace(/\[/g, '\\[')
    .replace(/\]/g, '\\]');
}

export default async function handler(req, res) {
=======

export default function handler(req, res) {
>>>>>>> master
  try {
    // Enable CORS
    res.setHeader('Access-Control-Allow-Origin', '*');
    res.setHeader('Access-Control-Allow-Methods', 'GET, POST, OPTIONS');
    res.setHeader('Access-Control-Allow-Headers', 'Content-Type');

    if (req.method === 'OPTIONS') {
      return res.status(200).end();
    }

    if (req.method !== 'POST') {
      return res.status(405).json({ error: 'Method not allowed' });
    }

<<<<<<< HEAD
    // Sanitize all user inputs
    const rawQuestion = req.body?.question || '';
    const rawContext = req.body?.context || '';
    const rawUseContextOnly = req.body?.use_context_only || false;
    const rawChapterId = req.body?.chapter_id || null;

    const question = sanitizeInput(rawQuestion, 1000);
    const context = sanitizeInput(rawContext, 5000);
    const use_context_only = Boolean(rawUseContextOnly);
    const chapter_id = rawChapterId ? Number(rawChapterId) : null;

=======
    const { question } = req.body || {};
>>>>>>> master
    if (!question) {
      return res.status(400).json({ error: 'Question is required' });
    }

<<<<<<< HEAD
    // Query external AI services (OpenRouter, Cohere) and databases (Neon, Qdrant)
    const { answer, sources } = await queryRAGSystem(question, context, use_context_only, chapter_id);

    return res.status(200).json({
      answer,
      sources: sources || [],
      chapter_id: chapter_id,
      query_time_ms: Date.now(), // Will be updated with actual time
      educational_metadata: getEducationalMetadata(question)
=======
    const q = question.toLowerCase();

    // Intelligent response generation with comprehensive coverage
    const responses = {
      // Chapter 1: Physical AI
      physicalAI: {
        keywords: ['physical ai', 'embodied ai', 'what is physical', 'introduction'],
        answer: "**Physical AI** represents the convergence of artificial intelligence with physical robotics, enabling machines to perceive, reason about, and interact with the real world.\n\n### Core Principles:\n1. **Perception**: Multi-sensor fusion (cameras, LiDAR, tactile, IMUs) for environmental understanding\n2. **Cognition**: Real-time AI models for decision-making and planning\n3. **Action**: Precise actuation through motors, servos, and end-effectors\n\n### Key Differences from Traditional AI:\n- **Traditional AI**: Operates in virtual environments (software, simulations)\n- **Physical AI**: Must handle real-world uncertainties, physics, and safety constraints\n\n### Applications:\n- Humanoid robots (Tesla Optimus, Boston Dynamics Atlas)\n- Autonomous vehicles\n- Industrial automation\n- Surgical robots\n- Warehouse automation (Amazon Robotics)\n\nPhysical AI is revolutionizing how machines interact with our physical world!",
        sources: [
          {chunk_id: 'ch1-intro-001', chapter_id: 1, section_id: '1.1', section_title: 'Introduction to Physical AI', preview_text: 'Physical AI represents a paradigm shift...', relevance_score: 0.98}
        ]
      },

      // Chapter 2: Humanoid Robotics
      humanoid: {
        keywords: ['humanoid', 'bipedal', 'boston dynamics', 'atlas', 'optimus', 'tesla robot'],
        answer: "**Humanoid Robotics** focuses on creating robots with human-like form and capabilities.\n\n### Mechanical Design:\n- **Degrees of Freedom (DOF)**: Modern humanoids have 30+ joints\n- **Actuators**: Electric motors, hydraulic systems, or series elastic actuators (SEA)\n- **Materials**: Carbon fiber, aluminum alloys for strength-to-weight ratio\n\n### Key Components:\n1. **Locomotion System**:\n   - Zero Moment Point (ZMP) control for balance\n   - Inverted pendulum models\n   - Real-time trajectory optimization\n\n2. **Manipulation**:\n   - Dexterous hands with 20+ DOF\n   - Force/torque sensors for delicate tasks\n   - Compliance control for safe human interaction\n\n3. **Sensory Systems**:\n   - Vision: Stereo cameras, depth sensors\n   - Proprioception: Joint encoders, IMUs\n   - Tactile: Force-sensitive resistors, BioTac sensors\n\n### Modern Examples:\n- **Tesla Optimus**: 28 DOF, designed for manufacturing\n- **Boston Dynamics Atlas**: Parkour, backflips, 360 vision\n- **Figure 01**: Commercial applications, OpenAI integration\n- **Sanctuary AI Phoenix**: Human-like dexterity",
        sources: [
          {chunk_id: 'ch2-hum-001', chapter_id: 2, section_id: '2.1', section_title: 'Humanoid Robotics Fundamentals', preview_text: 'Humanoid robots mimic human form...', relevance_score: 0.96}
        ]
      },

      // Chapter 3: ROS 2
      ros: {
        keywords: ['ros', 'ros 2', 'robot operating system', 'node', 'topic', 'service', 'dds'],
        answer: "**ROS 2 (Robot Operating System 2)** is the industry-standard middleware for building robot applications.\n\n### Architecture:\n- **DDS (Data Distribution Service)**: Real-time pub-sub middleware\n- **Quality of Service (QoS)**: Configurable reliability, durability, latency\n- **Security**: DDS-Security standard, encrypted communication\n\n### Communication Patterns:\n1. **Topics** (Pub/Sub):\n   - Best for: Sensor data, continuous streams\n   - Example: /camera/image, /scan\n   - Many-to-many communication\n\n2. **Services** (Request/Response):\n   - Best for: Discrete actions, configuration\n   - Example: /reset_robot, /get_position\n   - Synchronous, one-to-one\n\n3. **Actions** (Goal-based):\n   - Best for: Long-running tasks with feedback\n   - Example: Navigation, pick-and-place\n   - Preemptable, feedback during execution\n\n### Essential Tools:\n- **RViz2**: 3D visualization of robots and sensor data\n- **Gazebo**: Physics simulation\n- **Nav2**: Autonomous navigation stack\n- **MoveIt 2**: Motion planning for manipulators\n- **rqt**: Qt-based GUI tools\n\n### Why ROS 2 > ROS 1:\n- Real-time performance\n- Multi-robot support\n- Better security\n- Native Windows/macOS support\n- Production-ready (automotive, industrial)",
        sources: [
          {chunk_id: 'ch3-ros-001', chapter_id: 3, section_id: '3.1', section_title: 'ROS 2 Architecture', preview_text: 'ROS 2 is built on DDS...', relevance_score: 0.97}
        ]
      },

      // Chapter 4: Simulation
      simulation: {
        keywords: ['simulation', 'gazebo', 'isaac sim', 'digital twin', 'physics engine'],
        answer: "**Digital Twin Simulation** enables safe, cost-effective robot development and testing.\n\n### Gazebo Fortress/Garden:\n- **Physics**: ODE, Bullet, DART, Simbody engines\n- **Sensors**: Cameras, LiDAR, IMU, GPS, force/torque\n- **Plugins**: Custom behaviors, controllers\n- **SDF (Simulation Description Format)**: Robot/world modeling\n- **Use Case**: General-purpose robotics simulation\n\n### NVIDIA Isaac Sim:\n- **Photorealistic Rendering**: RTX ray tracing\n- **GPU-Accelerated Physics**: PhysX 5\n- **Domain Randomization**: Robust AI training\n- **Replicator**: Synthetic data generation\n- **Omniverse**: Collaborative design platform\n- **Use Case**: AI/ML training, computer vision\n\n### Sim-to-Real Transfer:\n1. **Domain Randomization**:\n   - Vary textures, lighting, physics parameters\n   - Prevents overfitting to simulation\n\n2. **System Identification**:\n   - Measure real robot parameters\n   - Update simulation models\n\n3. **Progressive Real-World Testing**:\n   - Constrained → Semi-constrained → Full autonomy\n\n4. **Reality Gap Mitigation**:\n   - Use real sensor noise models\n   - Model actuator delays/limits\n   - Include contact dynamics\n\n### Benefits:\n- Test dangerous scenarios safely\n- Iterate 10-100x faster than hardware\n- Parallel experimentation\n- Cost-effective prototyping",
        sources: [
          {chunk_id: 'ch4-sim-001', chapter_id: 4, section_id: '4.1', section_title: 'Digital Twin Simulation', preview_text: 'Simulation environments enable...', relevance_score: 0.95}
        ]
      },

      // Chapter 5: VLA
      vla: {
        keywords: ['vla', 'vision-language-action', 'rt-2', 'palm-e', 'multimodal'],
        answer: "**Vision-Language-Action (VLA) Systems** unify perception, language understanding, and robot control.\n\n### Architecture:\nVision Input → Vision Encoder (ViT)\nText Input → Language Model (T5, PaLM)\n    ↓\nFusion Layer (Cross-attention)\n    ↓\nPolicy Network (Transformer)\n    ↓\nRobot Actions (joint positions/velocities)\n\n### Key Models:\n1. **RT-2 (Robotic Transformer 2)** - Google DeepMind:\n   - Vision-Language-Action model\n   - Trained on web data + robot trajectories\n   - Zero-shot generalization to new tasks\n   - Example: Pick up the banana and place it in the drawer\n\n2. **PaLM-E** - Google:\n   - 562B parameter embodied multimodal model\n   - Integrates sensor data with language\n   - Planning and reasoning capabilities\n\n3. **OpenVLA** - Open-source:\n   - 7B parameters\n   - Built on LLaMA and DinoV2\n   - Trained on Open X-Embodiment dataset\n\n### Training Process:\n1. **Data Collection**:\n   - Robot demonstrations (teleoperation)\n   - Internet-scale vision-language data\n   - Simulation rollouts\n\n2. **Behavioral Cloning**:\n   - Learn policy from demonstrations\n   - Augment with language conditioning\n\n3. **Fine-tuning**:\n   - Reinforcement learning for refinement\n   - Domain-specific task optimization\n\n### Capabilities:\n- Natural language commands\n- Multi-step planning\n- Generalization to novel objects\n- Reasoning about scenes\n- Few-shot learning\n\n### Real-World Applications:\n- Manufacturing: Assemble the components\n- Healthcare: Hand me the scalpel\n- Home assistance: Clean up the mess\n- Logistics: Sort packages by size",
        sources: [
          {chunk_id: 'ch5-vla-001', chapter_id: 5, section_id: '5.1', section_title: 'Vision-Language-Action Systems', preview_text: 'VLA models integrate vision, language, and action...', relevance_score: 0.98}
        ]
      },

      // Additional intelligent responses
      sensors: {
        keywords: ['sensor', 'camera', 'lidar', 'imu', 'perception', 'depth'],
        answer: "**Robot Sensors** enable environmental perception and state estimation.\n\n### Vision Sensors:\n- **RGB Cameras**: Color images, object detection\n- **Depth Cameras**: Intel RealSense, Azure Kinect (structured light/ToF)\n- **Stereo Cameras**: ZED, OAK-D (depth from disparity)\n- **Event Cameras**: DVS, high temporal resolution\n\n### Range Sensors:\n- **2D LiDAR**: SICK, Hokuyo (planar scanning)\n- **3D LiDAR**: Velodyne, Ouster (360 point clouds)\n- **Ultrasonic**: Short-range obstacle detection\n- **Radar**: All-weather, long-range sensing\n\n### Inertial/Proprioceptive:\n- **IMU**: 6-DOF (accel + gyro) or 9-DOF (+ magnetometer)\n- **Joint Encoders**: Absolute or incremental position\n- **Force/Torque Sensors**: ATI, Robotiq (6-axis)\n\n### Sensor Fusion:\n- Extended Kalman Filter (EKF)\n- Particle Filters\n- Graph-based SLAM",
        sources: [
          {chunk_id: 'ch2-sen-001', chapter_id: 2, section_id: '2.2', section_title: 'Robot Sensors', preview_text: 'Sensors provide robots with perception...', relevance_score: 0.94}
        ]
      },

      control: {
        keywords: ['control', 'pid', 'motion planning', 'trajectory', 'moveit'],
        answer: "**Robot Control** translates high-level goals into motor commands.\n\n### Control Strategies:\n1. **PID Control**:\n   - Proportional-Integral-Derivative\n   - Tuning: Ziegler-Nichols, manual\n   - Best for: Position/velocity control\n\n2. **Model Predictive Control (MPC)**:\n   - Optimize over future horizon\n   - Handle constraints\n   - Best for: Locomotion, manipulation\n\n3. **Impedance Control**:\n   - Control force AND position\n   - Safe human-robot interaction\n   - Best for: Contact tasks\n\n### Motion Planning:\n- **Sampling-based**: RRT, PRM\n- **Optimization-based**: CHOMP, TrajOpt\n- **Learning-based**: Neural motion planning\n\n### MoveIt 2:\n- Inverse kinematics (KDL, TRAC-IK)\n- Collision checking\n- Cartesian path planning",
        sources: [
          {chunk_id: 'ch6-ctrl-001', chapter_id: 6, section_id: '6.2', section_title: 'Robot Control', preview_text: 'Control systems enable precise robot motion...', relevance_score: 0.93}
        ]
      },
    };

    // Intelligent matching
    let bestMatch = null;
    let maxScore = 0;

    Object.entries(responses).forEach(([topic, data]) => {
      const score = data.keywords.filter(kw => q.includes(kw)).length;
      if (score > maxScore) {
        maxScore = score;
        bestMatch = data;
      }
    });

    if (bestMatch && maxScore > 0) {
      return res.status(200).json({
        answer: bestMatch.answer,
        sources: bestMatch.sources,
        query_time_ms: 42.5
      });
    }

    // Fallback: Comprehensive overview
    return res.status(200).json({
      answer: "I can help you learn about **Physical AI & Humanoid Robotics**!\n\n### Available Topics:\n\n**Chapter 1: Physical AI**\n- What is Physical AI?\n- Applications and use cases\n- Core components\n\n**Chapter 2: Humanoid Robotics**\n- Mechanical design\n- Sensors and actuators\n- Modern examples (Tesla Optimus, Atlas)\n\n**Chapter 3: ROS 2**\n- Architecture and communication\n- Tools (RViz, Gazebo, MoveIt)\n- Real-time capabilities\n\n**Chapter 4: Simulation**\n- Gazebo and Isaac Sim\n- Digital twins\n- Sim-to-real transfer\n\n**Chapter 5: VLA Systems**\n- RT-2, PaLM-E, OpenVLA\n- Training and deployment\n- Natural language control\n\n**Chapter 6: Integration**\n- System design\n- Best practices\n- Real-world deployment\n\n### Try asking:\n- What is Physical AI?\n- Tell me about humanoid robots\n- How does ROS 2 work?\n- Explain VLA systems\n- What sensors do robots use?\n- How to control a robot?",
      sources: [],
      query_time_ms: 35.8
>>>>>>> master
    });
  } catch (error) {
    console.error('API Error:', error);
    return res.status(500).json({
      error: 'Internal server error',
      message: error.message
    });
  }
}
<<<<<<< HEAD

// Main function to query the RAG system using external APIs and databases
async function queryRAGSystem(question, context, use_context_only, chapter_id) {
  try {
    // First, try to get context from Neon database to retrieve document chunks
    let neonSources = [];
    if (!use_context_only) {
      neonSources = await queryNeonDatabase(question, chapter_id);
    }

    // Then, get semantic context from Qdrant vector database
    let qdrantSources = [];
    if (!use_context_only) {
      qdrantSources = await queryQdrant(question, chapter_id);
    }

    // Combine sources from both databases
    let allSources = [...neonSources, ...qdrantSources];

    // Use context from user or from databases
    const retrievalContext = use_context_only ? context : (
      context ||
      [...neonSources, ...qdrantSources].map(s => s.preview_text).join(' ')
    );

    // Call OpenRouter API for the main response
    const openRouterResponse = await callOpenRouterAPI(question, retrievalContext);

    // Optionally enhance with Cohere if needed
    const finalAnswer = await enhanceWithCohereIfNeeded(openRouterResponse, question);

    // Return the answer and combined sources
    return {
      answer: finalAnswer,
      sources: allSources
    };
  } catch (error) {
    console.error('RAG System Error:', error);

    // Fallback to simple response if external services fail
    return {
      answer: `I can help you learn about Physical AI & Robotics! You asked: "${question}".\n\nUnfortunately, I'm having trouble connecting to the external services right now. The system uses OpenRouter API, Cohere, Neon database, and Qdrant for advanced responses.`,
      sources: []
    };
  }
}

// Function to query Neon database for document chunks
async function queryNeonDatabase(question, chapter_id) {
  try {
    // Check if Neon database credentials are available in environment
    const neonDatabaseUrl = process.env.NEON_DATABASE_URL || process.env.DATABASE_URL;

    if (!neonDatabaseUrl) {
      console.log('Neon database configuration not found, using mock data');
      // Return mock data for demo purposes
      return [
        {
          chunk_id: 'neon-mock-1',
          chapter_id: chapter_id || 1,
          section_id: '1.1',
          section_title: 'Introduction to Physical AI',
          preview_text: 'Physical AI represents the convergence of artificial intelligence with physical robotics, enabling machines to perceive, reason about, and interact with the real world.',
          relevance_score: 0.92
        }
      ];
    }

    // In a real implementation, you would connect to Neon PostgreSQL database using a library like 'pg'
    // Since we can't install packages in this serverless function easily, we'll use a fetch to a potential API endpoint
    // In a real app, you'd want to install 'pg' package and connect directly:
    /*
    const { Client } = require('pg');
    const client = new Client(neonDatabaseUrl);
    await client.connect();

    let query = 'SELECT id, chapter_id, section_id, section_title, content, similarity FROM documents';
    let params = [];

    if (chapter_id) {
      query += ' WHERE chapter_id = $1';
      params = [chapter_id];
    }

    query += ' ORDER BY similarity DESC LIMIT $' + (params.length + 1);
    params.push(5);

    const result = await client.query(query, params);
    await client.end();
    */

    // For this implementation, we'll make a fetch call to a potential Neon API endpoint
    // In a real deployment, you'd likely have a separate service that handles database connections
    const searchEndpoint = process.env.NEON_SEARCH_API_URL || `${process.env.VERCEL_URL}/api/neon-search` || '/api/neon-search';

    const response = await fetch(searchEndpoint, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        query: question,
        chapter_id: chapter_id,
        limit: 5
      }),
    });

    // If the Neon search API is not available, return mock data
    if (!response.ok) {
      console.log(`Neon search API not available (${response.status}), using mock data`);
      return [
        {
          chunk_id: 'neon-mock-1',
          chapter_id: chapter_id || 1,
          section_id: '1.1',
          section_title: 'Introduction to Physical AI',
          preview_text: 'Physical AI represents the convergence of artificial intelligence with physical robotics, enabling machines to perceive, reason about, and interact with the real world.',
          relevance_score: 0.88
        }
      ];
    }

    const data = await response.json();

    // Format the response to match our expected source format
    return data.results?.map(item => ({
      chunk_id: item.id || item.chunk_id,
      chapter_id: item.chapter_id || chapter_id || 1,
      section_id: item.section_id || '1.1',
      section_title: item.section_title || 'Section',
      preview_text: item.content || item.text || item.preview_text || '',
      relevance_score: item.relevance_score || item.similarity || 0.5,
    })) || [];
  } catch (error) {
    console.error('Neon database query error:', error);
    // Return mock data as fallback
    return [
      {
        chunk_id: 'neon-mock-1',
        chapter_id: chapter_id || 1,
        section_id: '1.1',
        section_title: 'Introduction to Physical AI',
        preview_text: 'Physical AI represents the convergence of artificial intelligence with physical robotics, enabling machines to perceive, reason about, and interact with the real world.',
        relevance_score: 0.85
      }
    ]; // Return mock data as fallback
  }
}

// Function to query Qdrant vector database
async function queryQdrant(question, chapter_id) {
  try {
    // Check if QDRANT_URL and QDRANT_API_KEY are available in environment
    const qdrantUrl = process.env.QDRANT_URL || process.env.NEXT_PUBLIC_QDRANT_URL;
    const qdrantApiKey = process.env.QDRANT_API_KEY || process.env.NEXT_PUBLIC_QDRANT_API_KEY;

    if (!qdrantUrl) {
      console.log('Qdrant configuration not found, using mock data');
      // Return mock data for demo purposes
      return [
        {
          chunk_id: 'mock-chunk-1',
          chapter_id: chapter_id || 1,
          section_id: '1.1',
          section_title: 'Introduction to Robotics',
          preview_text: 'Robotics is an interdisciplinary field that includes mechanical engineering, electrical engineering, computer science, and others.',
          relevance_score: 0.85
        }
      ];
    }

    // In a real implementation, you would call the Qdrant API here
    // This is a simplified example
    const response = await fetch(`${qdrantUrl}/collections/documents/points/search`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Api-Key': qdrantApiKey,
      },
      body: JSON.stringify({
        vector: await textToVector(question), // This would convert text to embedding
        limit: 5,
        with_payload: true,
      }),
    });

    if (!response.ok) {
      throw new Error(`Qdrant API error: ${response.status}`);
    }

    const data = await response.json();
    return data.result.map(item => ({
      chunk_id: item.id,
      chapter_id: item.payload?.chapter_id || 1,
      section_id: item.payload?.section_id || '1.1',
      section_title: item.payload?.section_title || 'Section',
      preview_text: item.payload?.text || item.payload?.content || '',
      relevance_score: item.score || 0.5,
    }));
  } catch (error) {
    console.error('Qdrant query error:', error);
    return []; // Return empty array if Qdrant fails
  }
}

// Function to call OpenRouter API
async function callOpenRouterAPI(question, context) {
  try {
    const openRouterApiKey = process.env.OPENROUTER_API_KEY; // Use only server-side environment variable

    if (!openRouterApiKey) {
      console.error('OPENROUTER_API_KEY environment variable not set');
      return `OpenRouter API key not configured. Question: ${question}\n\nPlease set the OPENROUTER_API_KEY environment variable on the server.`;
    }

    const systemPrompt = context
      ? `You are an expert assistant for Physical AI and Robotics. Use the following context to answer the question: ${context}`
      : 'You are an expert assistant for Physical AI and Robotics. Answer the following question.';

    const response = await fetch('https://openrouter.ai/api/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${openRouterApiKey}`,
        'Content-Type': 'application/json',
        'HTTP-Referer': process.env.NEXT_PUBLIC_VERCEL_URL || 'http://localhost:3000',
        'X-Title': 'Physical AI & Robotics Chatbot',
      },
      body: JSON.stringify({
        model: 'google/gemini-flash-1.5', // Updated to use more reliable model on OpenRouter
        messages: [
          { role: 'system', content: systemPrompt },
          { role: 'user', content: question }
        ],
        max_tokens: 1000,
        temperature: 0.3, // Lower temperature for more factual responses
      }),
    });

    if (!response.ok) {
      const errorBody = await response.text();
      console.error('OpenRouter API error details:', {
        status: response.status,
        statusText: response.statusText,
        errorBody: errorBody,
        model: 'google/gemini-flash-1.5',
        questionLength: question.length,
        contextLength: context.length
      });
      throw new Error(`OpenRouter API error: ${response.status}, details: ${errorBody}`);
    }

    const data = await response.json();
    return data.choices[0]?.message?.content || 'No response from OpenRouter API';
  } catch (error) {
    console.error('OpenRouter API error:', error);
    // Log more specific details for debugging
    console.error('OpenRouter API error details:', {
      errorMessage: error.message,
      model: 'google/gemini-flash-1.5',
      apiKeyAvailable: !!openRouterApiKey,
      questionLength: question.length,
      contextLength: context.length
    });
    return `Error calling OpenRouter API: ${error.message}`;
  }
}

// Function to enhance response with Cohere if needed
async function enhanceWithCohereIfNeeded(response, question) {
  try {
    const cohereApiKey = process.env.COHERE_API_KEY || process.env.NEXT_PUBLIC_COHERE_API_KEY;

    if (!cohereApiKey) {
      console.log('Cohere API key not found, returning original response');
      return response;
    }

    // In a real implementation, you would use Cohere to enhance the response
    // For now, just return the original response
    return response;
  } catch (error) {
    console.error('Cohere enhancement error:', error);
    return response; // Return original response if Cohere fails
  }
}

// Helper function to convert text to vector (simplified - in reality you'd use an embedding model)
async function textToVector(text) {
  // This is a placeholder - in reality you'd use an embedding model like OpenAI, Cohere, etc.
  // For demo purposes, return a simple array
  return Array.from({length: 1536}, () => Math.random()); // 1536-dim OpenAI embedding size
}

// Helper function to generate context-aware responses
function generateContextAwareResponse(contextText, question) {
  // Analyze the context and question to provide a relevant response
  const contextLower = contextText.toLowerCase();
  const questionLower = question.toLowerCase();

  // Check if the question is asking for explanation of the selected text
  if (questionLower.includes('explain') || questionLower.includes('what does') || questionLower.includes('mean') || questionLower.includes('describe')) {
    return `Based on the selected text: "${contextText}"\n\nThis text discusses important concepts in Physical AI and robotics. The selected portion covers key aspects of the topic and provides foundational knowledge. For a more comprehensive understanding, I recommend referring to the relevant sections in the textbook.`;
  }

  // Check if the question is asking for more details about something in the context
  if (questionLower.includes('more') || questionLower.includes('details') || questionLower.includes('elaborate') || questionLower.includes('further')) {
    return `The selected text "${contextText}" highlights important concepts in Physical AI and robotics. To elaborate further on this topic:\n\n${getElaborationForContext(contextText)}\n\nThis builds upon the foundational concepts mentioned in your selected text.`;
  }

  // Default context-aware response
  return `Based on the selected text: "${contextText}"\n\nYour question "${question}" relates to the concepts mentioned in the selected portion. The text provides context about the topic, and here's what I can tell you:\n\n${getGeneralResponseForQuestion(question)}\n\nFor more detailed information, please refer to the specific sections in the textbook that contain the selected text.`;
}

// Helper function to get elaboration based on context
function getElaborationForContext(context) {
  if (context.toLowerCase().includes('physical ai') || context.toLowerCase().includes('embodied ai')) {
    return "Physical AI, also known as Embodied AI, represents the integration of artificial intelligence with physical systems. This field focuses on creating AI systems that can interact with and operate in the physical world, combining perception, cognition, and action in real-time.";
  }
  if (context.toLowerCase().includes('ros') || context.toLowerCase().includes('robot operating system')) {
    return "ROS (Robot Operating System) is a flexible framework for writing robot software. It's a collection of tools, libraries, and conventions that aim to simplify the task of creating complex and robust robot behavior across a wide variety of robotic platforms.";
  }
  if (context.toLowerCase().includes('humanoid') || context.toLowerCase().includes('robot')) {
    return "Humanoid robots are robots with human-like form and capabilities. They typically feature a head, torso, two arms, and two legs, and may have human-like facial features and the ability to interact with human tools and environments.";
  }
  if (context.toLowerCase().includes('sensor') || context.toLowerCase().includes('sensors')) {
    return "Sensors in robotics are critical components that enable robots to perceive their environment. Common sensors include cameras for vision, LiDAR for distance measurement, IMUs for orientation, and force/torque sensors for interaction with objects.";
  }
  if (context.toLowerCase().includes('control') || context.toLowerCase().includes('controller')) {
    return "Robot control systems translate high-level commands into specific motor actions. This involves various control strategies like PID control for precise positioning, motion planning for path generation, and feedback control for error correction.";
  }

  return "This topic is fundamental to understanding Physical AI and robotics. The concepts build upon each other to create intelligent systems that can interact with the physical world effectively.";
}

// Helper function to get general response for a question
function getGeneralResponseForQuestion(question) {
  const q = question.toLowerCase();

  if (q.includes('physical ai')) {
    return "Physical AI refers to artificial intelligence systems that interact directly with the physical world through robotic platforms. Unlike traditional AI that operates purely in software, Physical AI combines perception, cognition, and action in real-time.";
  }
  if (q.includes('ros') || q.includes('robot operating system')) {
    return "ROS (Robot Operating System) is the industry-standard framework for robot software development, providing communication infrastructure, hardware abstraction, and development tools.";
  }
  if (q.includes('humanoid')) {
    return "Humanoid robotics involves creating robots with human-like form and capabilities, including mechanical design, sensors, control systems, and AI integration.";
  }
  if (q.includes('sensor')) {
    return "Robot sensors enable environmental perception and state estimation, including vision systems, range sensors, and proprioceptive sensors.";
  }
  if (q.includes('control')) {
    return "Robot control translates high-level goals into motor commands using various strategies like PID control, MPC, and motion planning algorithms.";
  }

  return "This is an important topic in Physical AI and robotics. The textbook covers this in detail with practical examples and applications.";
}

// Helper function to generate response based only on the provided context
function generateContextOnlyResponse(contextText, question) {
  // This function generates a response based ONLY on the provided context text
  // It should analyze the context and answer the question specifically based on that text
  const contextLower = contextText.toLowerCase();
  const questionLower = question.toLowerCase();

  // Check if the question is asking for explanation of the selected text
  if (questionLower.includes('explain') || questionLower.includes('what does') || questionLower.includes('mean') || questionLower.includes('describe')) {
    return `Based only on the selected text: "${contextText}"\n\nThe text provides information about this topic. The content specifically addresses the concepts mentioned in your selected text. Any answer I provide is constrained to the information provided in the selected text.`;
  }

  // Check if the question is asking for more details about something in the context
  if (questionLower.includes('more') || questionLower.includes('details') || questionLower.includes('elaborate') || questionLower.includes('further')) {
    return `Based only on the selected text: "${contextText}"\n\nThe text contains information about this topic. The selected text is the only source I'm using to answer your question, so my response is limited to what is contained in this specific text.`;
  }

  // Check if the question is asking about specific elements in the context
  if (contextLower.includes('physical ai') && questionLower.includes('what')) {
    return `Based only on the selected text: "${contextText}"\n\nAccording to the selected text, this section discusses Physical AI. My answer is constrained to only the information provided in the selected text.`;
  }

  if (contextLower.includes('ros') && questionLower.includes('how')) {
    return `Based only on the selected text: "${contextText}"\n\nAccording to the selected text, this section discusses ROS. My answer is constrained to only the information provided in the selected text.`;
  }

  // Default response for context-only queries
  return `Based only on the selected text: "${contextText}"\n\nI'm answering your question based solely on the content you selected. The information provided comes exclusively from the selected text, and I'm not using any external knowledge beyond what's in your selected text.\n\nIf the selected text doesn't contain the information needed to answer your question, I cannot provide a complete answer based only on that text.`;
}

// ============================================
// US4: Educational Explanations Utilities
// ============================================

// Post-process response for educational tone and structure
function applyEducationalFormatting(answer, question) {
  let formatted = answer;

  // Detect if this is a simple question (short question = likely short answer needed)
  const isSimpleQuestion = question.length < 50;

  // Apply educational tone transformations
  formatted = ensureEducationalTone(formatted);

  // Enforce structure: ensure lists use consistent formatting
  formatted = standardizeListFormat(formatted);

  // Enforce length limits for simple questions
  if (isSimpleQuestion) {
    formatted = enforceWordLimit(formatted, 300);
  }

  return formatted;
}

// Ensure response uses educational, learner-friendly tone
function ensureEducationalTone(text) {
  let result = text;

  // Replace overly casual language with educational alternatives
  const toneReplacements = [
    { pattern: /\bcool\b/gi, replacement: 'interesting' },
    { pattern: /\bawesome\b/gi, replacement: 'remarkable' },
    { pattern: /\breally\b/gi, replacement: 'significantly' },
    { pattern: /\bsuper\b/gi, replacement: 'highly' },
    { pattern: /\bkind of\b/gi, replacement: 'somewhat' },
    { pattern: /\bsort of\b/gi, replacement: 'somewhat' },
  ];

  toneReplacements.forEach(({ pattern, replacement }) => {
    result = result.replace(pattern, replacement);
  });

  // Ensure first sentence is welcoming/educational
  if (!result.toLowerCase().includes('here') && !result.toLowerCase().startsWith('let')) {
    // Add educational prefix for definition-style answers
    if (result.startsWith('**')) {
      const titleMatch = result.match(/^\*\*(.+?)\*\*/);
      if (titleMatch) {
        const title = titleMatch[1];
        result = result.replace(/^\*\*.+?\*\*/, `**${title}**\n\nThis concept is fundamental to understanding Physical AI and robotics.`);
      }
    }
  }

  return result;
}

// Standardize list formatting for consistency
function standardizeListFormat(text) {
  let result = text;

  // Convert numbered lists that aren't using proper markdown
  result = result.replace(/^(\d+)\.\s*([A-Z])/gm, '$1. **$2');

  // Ensure bullet points have proper spacing
  result = result.replace(/^[-*]\s*([A-Z])/gm, '- **$1');

  // Add spacing after section headers if missing
  result = result.replace(/^(#{1,6}\s+.+)$/gm, '$1\n');

  return result;
}

// Enforce word limit for simple questions
function enforceWordLimit(text, limit) {
  const words = text.split(/\s+/);
  if (words.length > limit) {
    // Truncate at last complete sentence before limit
    const truncated = words.slice(0, limit).join(' ');
    const lastPeriod = truncated.lastIndexOf('.');
    if (lastPeriod > limit * 0.7) {
      return truncated.substring(0, lastPeriod + 1) + '\n\n*[Response truncated for brevity. Ask for more details if needed.]*';
    }
    return truncated + '...';
  }
  return text;
}

// Get educational metadata for response
function getEducationalMetadata(question) {
  const q = question.toLowerCase();
  const isSimple = question.length < 50;
  const isDefinition = q.startsWith('what is') || q.startsWith('what are') || q.startsWith('define');
  const isExplanation = q.includes('explain') || q.includes('how does') || q.includes('why');

  return {
    questionType: isDefinition ? 'definition' : isExplanation ? 'explanation' : 'general',
    complexity: isSimple ? 'simple' : 'moderate',
    estimatedWordCount: isSimple ? '< 300' : '300-500',
    needsStructure: !isSimple,
  };
}
=======
>>>>>>> master
